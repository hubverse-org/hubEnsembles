---
title: "`hubEnsembles`: Ensembling Methods in R"
output: 
bibliography: ../vignettes/references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 9
)
```

# Introduction

Predictions of future outcomes are essential to planning and decision making. Despite their utility, generating reliable predictions of the future is challenging. One method for overcoming this challenge is combining predictions across multiple, independent models. These combination methods (also called aggregation or ensembling) have been repeatedly shown to produce predictions that are more accurate [@clemen1989; @timmermann2006b] and more consistent [@hibon2005] than individual models. Because of the clear performance benefits, multi-model ensembles are commonplace across fields, including weather [@alley2019], climate [@tebaldi2007], and economics [@aastveit2018]. More recently, multi-model ensembles have been used to improve predictions of infectious disease outbreaks, including for respiratory viruses (influenza [@mcgowan2019], SARS-CoV-2 [@cramer2022]), vector borne diseases (Dengue [@johansson2019]), and hemorrhagic fevers (Ebola [@viboud2018]). <!--#EH: I DON'T LOVE THE EMPHASIS ON DISEASE APPLICATIONS… THOUGHTS ON WHETHER TO LEAVE THIS SENTENCE, SIMPLIFY IT, OR CUT IT?--> <!--# LS: We could include infectious diseases in the list of different fields that use ensembles, then move this sentence further down to around the mention of the hubverse since its goal and our main expertise lies in infectious disease forecasting.-->
<!--# ELR: I think the sentence is fine, but we could also combine it with the previous sentence: "Because of the clear performance benefits, multi-model ensembles are commonplace across fields, including weather [@alley2019], climate [@tebaldi2007], economics [@aastveit2018], and infectious disease [@mcgowan2019, @cramer2022, @johansson2019, @viboud2018])."   (Optionally picking a subset of the 4 infectious disease papers to cite)-->

Across this vast literature, there are many proposed methods for generating ensembles. Generally, these methods differ in at least one of two ways: (1) the function used to combine or "average" predictions, and (2) how predictions are weighted when performing the combination. No one method is universally "the best"; a simple average of predictions works surprisingly well across a range of settings [@winkler2015], but more complex approaches have also been shown to have benefits [REF]. Here, we present the `hubEnsembles` package, which provides a flexible framework for generating ensemble predictions from multiple models. Complementing an existing R package for generating ensembles of point estimates [@weiss2019], `hubEnsembles` supports multiple types of predictions, from point estimates to probabilistic predictions <!--#EH: DOES ANYONE KNOW OF OTHER PACKAGES?-->. <!--#ELR: I added some in this issue: https://github.com/Infectious-Disease-Modeling-Hubs/hubEnsembles/issues/43 -->

The `hubEnsembles` package is part of a larger collection of open-source software and data tools that enables collaborative forecasting exercises called the "hubverse". The broader "hubverse" initiative is motivated by the demonstrated benefits of collaborative hubs [@reich2022], including performance improvements of multi-model ensembles, and the desire for standardization across such hubs. In this paper, we focus specifically on the functionality encompassed in `hubEnsembles`. We provide an overview of the methods implemented, including mathematical definitions and properties (Section 2) as well as implementation details (Section 3), we give simple examples to demonstrate the functionality (Section 4) and a more complex case study (Section 5) that motivates a discussion and comparison of the various methods (Section 6).

<!--#EH GENERAL COMMENT: IF YOU HAVE BETTER REFERENCES ANYWHERE, LET ME KNOW.-->

# Mathematical definitions and properties of ensemble methods

The `hubEnsembles` package supports both point predictions and probabilistic predictions of different formats. A point prediction is a single estimate of a future outcome, and a probabilistic prediction gives an estimated probability distribution over future outcomes. <!--#ELR: I removed notation in this sentence. I had two goals: (1) I felt that x was being used in two different ways (as a point prediction and then as a possible value of the target variable); and (2) We used f(x) to talk about a probability distribution, but below we use $F(x)$; I thought we could just wait and introduce the notation we really want when we get there. --> We use $N$ to denote the total number of individual predictions that the ensemble will combine. For example, these predictions will often be produced by different statistical or mathematical models, and $N$ is the total number of models. Individual predictions will be indexed by the subscript $i$. Optionally, the package allows for calculating ensembles that use a weight $w_i$ for each prediction. Informally, predictions with a larger weight have a greater influence on the value of the ensemble prediction, though the details of this depend on the ensemble method (described more below). <!--#ELR: Trying to collect some stuff about notation up here. I saw similar descriptions of i, N, and w_i for both point forecasts and distributional forecasts below, thought we could just put it in one place up here. -->

For a set of point predictions, $x_i$ each from a distinct model $i$, the `hubEnsembles` package can compute an ensemble of these predictions

$$
x_E = C(x_i, w_i) 
$$

using any function $C$, and model-specific weights $w_i$. For example, an arithmetic average of predictions yields $x_E = \sum_{i=1}^Nx_iw_i$, where the weights satisfy $w_i \geq 0$ for all $i$ and $\sum_{i=1}^Nw_i=1$. <!--#ELR: or maybe clearer to just say "the weights are non-negative and sum to 1." --> If $w_i = 1/N$ for all $i$, all predictions will be equally weighted. This framework can also support more complex functions for aggregation, such as a (weighted) median or geometric mean.

For probabilistic predictions, there are two commonly used classes of methods to average or ensemble multiple predictions: quantile averaging [@lichtendahl2013] (also called a Vincent average [@vincent1912]) and probability averaging [@lichtendahl2013] (also called a distributional mixture [REF?] or linear opinion pool [@stone1961]). <!--#ELR: I updated the previous sentence to try to avoid implying that there are only 2 ways to combine predictions; e.g., we do not discuss logarithmic opinion pools. --> Let $F(x)$ be a cumulative density function (CDF) defined over values $x$ of the target variable for the prediction, and $F^{-1}(\theta)$ be the corresponding quantile function defined over quantile levels $\theta \in [0, 1]$. Additionally, we will use $f(x)$ to denote a probability mass function (PMF) for a prediction of a discrete variable or a discretization (such as binned values) of a continuous variable.

The quantile average is calculated as
$$
F^{-1}_Q(\theta) = \sum_{i = 1}^Nw_iF^{-1}_i(\theta).
$$
This computes the average value of predictions across different models for each fixed quantile level $\theta$. It is also possible to use other combination functions, such as a weighted median, to combine quantile predictions.

The probability average or linear pool is calculated by averaging probabilities across predictions for a fixed value of the target variable, $x$. This can be expressed in terms of either predictive CDFs or PMFs as follows:
\begin{align*}
F_{LOP}(x) &= \sum_{i = 1}^Nw_iF_i(x), \\
f_{LOP}(x) &= \sum_{i = 1}^Nw_if_i(x).
\end{align*}


The different averaging methods for probabilistic predictions yield different properties of the resulting ensemble distribution. For example, the variance of the linear pool is $\sigma^2_{LOP} = \sum_{i=1}^Nw_i\sigma_i^2 + \sum_{i=1}^Nw_i(\mu_i-\mu_{LOP})^2$, where $\mu_i$ is the mean and $\sigma^2_i$ is the variance of individual prediction $i$, and although there is no closed-form variance for the quantile average, the variance of the quantile average will always be less than or equal to that of the linear pool [@lichtendahl2013]. The linear pool method preserves variation between individual models, whereas the quantile average cancels away this variation [@howerton2023]. Both methods generate distributions with the same mean, $\mu_Q = \mu_{LOP} = \sum_{i=1}^Nw_i\mu_i$, which is the mean of individual model means. <!--#ELR: I didn't know this result for \mu_Q! Can we add a citation or put a derivation in a supplement? -->

# Model implementation details

To understand how these methods are implemented in `hubEnsembles`, we first must define the conventions employed by the hubverse and its packages for representing and working with model predictions. We begin with a short overview of concepts and conventions needed to utilize the `hubEnsembles` package, then explain the implementation of the two ensembling functions provided by the package, `simple_ensemble` and `linear_pool`.

## Hubverse terminology and conventions

Model output is a central concept in the `hubEnsembles` package. Model output generally refers to a specially formatted tabular representation of predictions produced by a modeling team. Each row represents a single, unique prediction with each column providing information about what is being predicted, its scope, and its value. Per hubverse convention, the columns may be broken down into task IDs, specification of the model output representation, and the model id [@hubverse_docs]. <!--#ELR: or `model_id`, if we want to name the specific column -->

At minimum the task IDs (also called task ID variables) together specify the desired outcome being predicted, but they may also include additional information, such as any conditions or assumptions that were used to generate the predictions [@hubverse_docs]. For example, forecasts of incident influenza hospitalizations in the US at different locations and amounts of time in the future might represent this information using a `target` column with the value "incident flu hospitalizations", a `location` column identifying the location being predicted, a `reference_date` column with the "starting point" of the forecasts, and a `horizon` column with the number of steps ahead that the forecast is predicting relative to the `reference_date`. All of these are task ID columns [@hubverse_docs]. <!--#ELR: Should we also provide a scenario projection example, to illustrate what we mean by the stuff about conditions or assumptions? -->
Different modeling efforts may use different sets of task ID columns and values to specify their prediction goals. Additional examples of task ID variables are available on the [hubverse documentation website](https://hubdocs.readthedocs.io/en/latest/format/tasks.html).

The model output representation includes the predicted values along with metadata that specifies how the predictions are conveyed, and consists of three columns: (1) `output_type`, (2) `output_type_id`, and (3) `value`. Unlike for the task IDs, these three columns are required and their names are fixed [@hubverse_docs]. The `output_type` defines how the prediction is represented and may be one of `"mean"` or `"median"` (point forecasts), `"quantile"`, `"cdf"`, or `"pmf"` (distributional forecasts). <!--#ELR: Should we mention "sample" output types too? --> The `output_type_id` provides more identifying information for a prediction and is specific to the particular `output_type`. For quantile predictions, the `output_type_id` is a numeric value between 0 and 1 specifying the probability level for the quantile. In the notation we defined above, the `output_type_id` corresponds to $\theta$ and the `value` of the prediction is the quantile estimate $F^{-1}(\theta)$.  For cdf or pmf predictions, the `output_type_id` is the value $x$ at which the cumulative distribution function or probability mass function for the predictive distribution should be evaluated, and the `value` column contains the estimate $F(x)$ or $f(x)$, respectively. Requirements for the values of the `output_type_id` and `value` columns associated with each valid output type are summarized [here](https://hubdocs.readthedocs.io/en/latest/user-guide/model-output.html#formats-of-model-output) in the hubverse documentation. 
<!--#ELR: This kind of citation seems fine for an ordinary package vignette, but if we're aiming to directly turn this into a journal submission, we'll need to tidy this citation up a bit. -->
<!--#ELR: I don't love the use of the word "estimate" that I introduced here -- noting that in much of statistics, a distinction is made between an estimate and a prediction, but I've used the two terms semi-interchangeably here. -->

<!-- # LS: Should we include the summary table from the hubverse docs in the paper itself or simply link it? -->
<!--#ELR: I think it could be helpful to include some kind of table here too.  It's a bit much to try to understand how the pieces fit together just from the paragraph in the text...? -->

Finally, the `model_id` column gives a unique identifier of the model that created the predictions. <!--#ELR: Note that I deleted some text here that implied that model_id was not a required column -- but currently, it looks like hubUtils::validate_model_out_tbl will throw an error if model_id is not in the tbl columns. -->

## Simple ensemble

The `simple_ensemble` function directly computes an ensemble from component model outputs by combining them via some function ($C$) <!--#ELR: sticking with notation for the combination function that was introduced earlier, and avoiding collision with the notation for a pmf --> within each unique combination of task ID variables, output types, and output type IDs. This function can be used to summarize predictions of output types mean, median, quantile, CDF, and PMF. The mechanics of the ensemble calculations are the same for each of the output types, though the resulting statistical ensembling method differs for different output types as described below. An aggregation function $C$ of choice may be specified by the user.

By default, `simple_ensemble` uses the mean for the aggregation function $C$ and equal weights for all models. For point predictions with a mean or median output type, the resulting ensemble prediction is an equally weighted average of the individual models' predictions. For probabilistic forecasts in a quantile format, by default `simple_ensemble` produces a quantile average, and for model outputs in a CDF or PMF format, by default `simple_ensemble` computes an equally weighted linear opinion pool.

A median ensemble may also be created by specifying "median" as the aggregation function, or a custom function may be passed to the `agg_fun` argument to create other ensemble types. Similarly, model weights can be specified to create a weighted ensemble. These weights are allowed to vary for different task ID values, and for quantile forecasts the weights may also be different for different quantile probability levels (corresponding to values of the `output_type_id` column).

## Linear pool

The `linear_pool` function implements the linear opinion pool method for ensembling projections. This function can be used to combine predictions with output types mean, quantile, CDF, and PMF. Unlike `simple_ensemble`, this function handles its computation differently based on the output type.

For the CDF, PMF, and mean output types, the linear pool method is equivalent to calling `simple_ensemble` with a mean aggregation function, since `simple_ensemble` produces a linear pool prediction for those output types. <!-- ELR: Commenting out the following text -- I moved the main point here up to the previous section about `simple_ensemble`, so that we could say what simple_ensemble did for all output types. Also, note that the statement about the mean is already present in the mathematical definitions section, so I think it's ok to jsut remove here.  "This (weighted) mean of component model outputs is the definitional calculation for a linear pool of a CDF or PMF (see [Mathematical definitions and properties of ensemble methods]), and the mean of the LOP is the (weighted) mean of the means of the component distributions, $\mu_{LOP} = \sum_{i=1}^Nw_i\mu_i$." -->

However, implementation of LOP is less straightforward for the quantile output type. The reason for this is that LOP averages CDF values (probabilities) at each value of the target variable, but the predictions are quantiles (on the scale of the target variable) at fixed probability levels. These quantiles will generally differ for different predictions, and as a result we typically are not provided with CDF values at the same values of $x$ for all component predictions. <!-- ELR: I personally find this to be confusing/non-obvious when I first start thinking about it. I tried to re-work this a bit to clarify, but I think it might be really helpful to include a figure somewhere to illustrate that if you are handed quantiles from different distributions at a fixed set of probability levels, you don't get to see the CDF values at the same places on the horizontal axis. --> Thus, we must first obtain an estimate of the CDF for each component distribution using the provided quantiles, combine the CDFs, then calculate the quantiles from the ensemble's CDF. We perform this calculation in three main steps, assisted by the `distfromq` package [REF] for the first two:

1.  Interpolate and extrapolate from the provided quantiles for each component model to obtain an estimate of the CDF of that particular distribution.
2.  Draw samples from each component model distribution. To reduce Monte Carlo variability, we use quasi-random samples corresponding to quantiles of the estimated distribution [@niederreiter1992quasirandom].
3.  Pool the samples from all component models and extract the desired quantiles.

For step 1, functionality in the `distfromq` package uses a monotonic cubic spline for interpolation on the interior of the provided quantiles. The user may choose one of several distributions to perform extrapolation of the CDF tails. These include normal, lognormal, and cauchy distributions, with "normal" set as the default. A location-scale parameterization is used, with separate location and scale parameters chosen in the lower and upper tails so as to match the two most extreme quantiles.

# Demonstration of functionality

In this section, we illustrate the two main functions in `hubEnsembles`, `simple_ensemble` and `linear_pool`. This vignette uses the following R packages:

```{r setup}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(cowplot)
library(hubUtils)
library(hubEnsembles)
```


## Example data: a simple forecast hub

The `example-simple-forecast-hub` has been created by the Consortium of Infectious Disease Modeling Hubs as a simple example hub to demonstrate the set up and functionality for the hubverse. The hub includes both example model output data and target data. The model output data includes `quantile`, `mean` and `median` forecasts of future incident hospitalizations, as well as `pmf` forecasts of the probability that the change in hospitalizations will be a "large decrease", "decrease", "stable", "increase", "large increase". 

<!-- # EH: A few notes on this: 
(1) I modified the example hub data to include mean and pmf output types. The mean output type was estimated from 1E4 samples (using distfromq package). The pmf output type was modeled after FluSight (probability of "large_decrease", "decrease", "stable", etc. but the values were randomly generated from a uniform distribution. Does this sound okay? Is it a problem that this won't match the example-hub directly?

(2) As of now, I have not included the cdf output type for inc hosp forecasts because it is difficult to define standard bins. I do, however, see value in including an example with cdf output type. So two potential options for that: 
    (a) include another target (e.g., peak timing) of cdf output type in this example hub 
    (b) create a very simple example, say with normal distributions, that will illustrate the fundamental differences between quantile average and linear pool (similar to Fig 2 in my paper, https://royalsocietypublishing.org/doi/10.1098/rsif.2022.0659) I have always found this figure useful, but maybe it is just me! I also think this option could help reinforce the connection between the methods described in the Mathematical Definitions section, and the implementation of each function (i.e., simple_ensemble() applied to quantile is a different method than simple_ensemble() applied to cdf/pmf).
) -->

<!-- # ELR:
(1) I think this is OK. We could grab updates forecasts (including categorical targets) from the new flu forecast hub if we wanted to, though. This would not be hard, but would require re-working some aspects of this set up?

(2) Your suggestion makes sense to me. We actually already have an example like this as part of the unit tests for the linear pool, here: https://github.com/Infectious-Disease-Modeling-Hubs/hubEnsembles/blob/80517854361312b81bce43dddf4d767d3a793b45/tests/testthat/test-linear_pool.R#L167.  I also find that plot helpful, and we might also be able to use it to help illustrate the issue with creating a linear pool based on quantile forecasts that I noted above.
-->

First we load the example model output data and the target data using the `connect_hub()` function from `hubUtils`, another package developed by the Consortium of Infectious Disease Modeling Hubs. 

```{r}
hub_path <- system.file("example-data/example-simple-forecast-hub",
                        package = "hubEnsembles")

model_outputs <- hubUtils::connect_hub(hub_path) %>%
  dplyr::collect()
head(model_outputs)

target_data_path <- file.path(hub_path, "target-data",
                              "covid-hospitalizations.csv")
target_data <- read.csv(target_data_path) %>%
    mutate(time_idx = as.Date(time_idx))
head(target_data)
```

## Creating ensembles with `simple_ensemble`

We can generate an equally weighted mean ensemble for each unique combination of values for the task ID variables (here, `origin_date`, `horizon`, `location`, and `target`), the `output_type` and the `output_type_id`. Note that this means that different ensemble methods are used for different output types: for the `quantile` output type in our example data, the resulting ensemble is a quantile average, while for the `pmf` output type, the ensemble is a linear pool.

```{r}
mean_ens <- hubEnsembles::simple_ensemble(model_outputs)
head(mean_ens)
```

### Changing the aggregation function

We can change the function that is used to aggregate model outputs. For example, we may want to calculate a median of component model submitted values for each quantile. We do so by specifying `agg_fun = median`. We will also use `model_id = "hub-ensemble-median` to change the name of this ensemble in the resulting data frame.

```{r}
median_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                            agg_fun = median, 
                                            model_id = "hub-ensemble-median")
head(median_ens)
```
Custom functions can also be passed into the `agg_fun` argument. For example, in some circumstances a geometric mean may be a more appropriate way to combine component model outputs; here we define a custom function `geometric_mean()` to do so. Any custom function to be used requires an argument `x` for the vector of numeric values to summarize, and if relevant, an argument `w` of numeric weights. Then, we can use this custom function to ensemble the component model outputs, again using `agg_fun = geometric_mean`.

```{r}
geometric_mean <- function(x){
    n <- length(x)
    return(prod(x)^(1/n))
}

geometric_mean_ens <-  hubEnsembles::simple_ensemble(model_outputs, 
                                                     agg_fun = geometric_mean, 
                                                     model_id = "hub-ensemble-geometric")
head(geometric_mean_ens)
```

As expected, the mean, median, and geometric mean each give us slightly different resulting ensembles. The median point estimates in Figure XX demonstrate this. 

```{r}
plt_all_point_ens <- bind_rows(mean_ens, 
                               median_ens, 
                               geometric_mean_ens) %>% 
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "median") %>%
    mutate(target_date =  origin_date + horizon, 
           ens_flag = model_id)

plt_all_point_mod <- model_outputs %>% 
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "median") %>%
    mutate(target_date =  origin_date + horizon)

ggplot(data = plt_all_point_ens) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_line(data = plt_all_point_mod, 
              aes(x = target_date, y = value, color = model_id), linewidth = 0.9) + 
    geom_line(aes(x = target_date, y = value, color = model_id), linewidth = 0.9) + 
    facet_grid(rows = vars(ens_flag)) +
    scale_color_manual(values = c(rainbow(3), gray(seq(0.2, 0.8,length.out = 3)))) +
    scale_y_continuous(labels = comma, 
                       name = "US incident hospitalizations") +
    theme_bw() + 
    theme(axis.title.x = element_blank(), 
          legend.position = "bottom", 
          legend.title = element_blank())
```

<!-- # EH: I don't love the color scheme here... it's especially hard to see the baseline model. -->

### Weighting model contributions

In addition, we can weight the contributions of each model by providing a `data.frame` that specifies these weights. For example, if we wanted to include the baseline model in the ensemble, but give it less weight than the other forecasts, we would use the `weights = model_weights` argument, where `model_weights` is a `data.frame` with a `model_id` column containing each unique model_id and a `weight` column.

```{r}
model_weights <- data.frame(model_id = c("UMass-ar", "UMass-gbq", "simple_hub-baseline"), 
                            weight = c(0.4, 0.4, 0.2))

weighted_mean_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                                   weights = model_weights, 
                                                   model_id = "hub-ensemble-weighted-mean")
head(weighted_mean_ens)
```
If the `data.frame` defines model weights in a column named something other than `weight`, we can specify this using the `weights_col_name` argument. <!-- ELR: describing this in the vignette feels low priority to me -- candidate for removal? -->

```{r}
model_weights <- data.frame(model_id = c("UMass-ar", "UMass-gbq", "simple_hub-baseline"), 
                            w = c(0.4, 0.4, 0.2))

weighted_mean_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                                   weights = model_weights, 
                                                   weights_col_name = "w",
                                                   model_id = "hub-ensemble-weighted-mean")
head(weighted_mean_ens)
```

## Creating ensembles with `linear_pool`

We can also generate a linear pool ensemble, or distributional mixture, using the `linear_pool()` function; this function can only be applied to predictions with an `output_type` of `mean`, `quantile`, `cdf`, or `pmf`. Our example hub includes `median` output type, so we exclude it from the calculation. 

```{r}
linear_pool_ens <- hubEnsembles::linear_pool(model_outputs %>%
                                                 filter(output_type != "median"), 
                                             model_id = "hub-ensemble-linear-pool")
head(linear_pool_ens)
```

As described above, for `quantile` model outputs, the `linear_pool` function approximates the full probability distribution for each component prediction using the value-quantile pairs provided by that model model, and then obtains quasi-random samples from that distributional estimate. The number of samples drawn from the distribution of each component model defaults to `1e4`, but this can be changed using the `n_samples` argument. 

For the `mean`, `cdf` and `pmf` output types, the linear pool is equivalent to using a mean `simple_ensemble` (see [Mathematical definitions and properties of ensemble methods] for explanation). For `quantile` output types, the `linear_pool` function yields an ensemble with wider prediction intervals than `simple_ensemble`. <!-- ELR: consider dropping this paragraph since it duplicates stuff that's been said before? -->

```{r}
plt_model_outputs = model_outputs %>%
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "quantile", 
           output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
    mutate(target_date =  origin_date + horizon, 
           output_type_id = paste0("Q", as.numeric(output_type_id)*100)) %>%
    spread(output_type_id, value)
    
plt_ens_outputs <- bind_rows(
    mean_ens %>% 
        filter(location == "US", 
               origin_date == "2022-12-12", 
               output_type == "quantile", 
               output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
        mutate(target_date =  origin_date + horizon, 
               output_type_id = paste0("Q", as.numeric(output_type_id)*100), 
               ens_fn = "simple_ensemble"), 
    linear_pool_ens %>% 
        filter(location == "US", 
               origin_date == "2022-12-12", 
               output_type == "quantile", 
               output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
        mutate(target_date =  origin_date + horizon, 
               output_type_id = paste0("Q", as.numeric(output_type_id)*100), 
               ens_fn = "linear_pool")
) %>%
    spread(output_type_id, value)

lims = c(min(c(plt_model_outputs$Q5, plt_ens_outputs$Q5)), 
         max(c(plt_model_outputs$Q95, plt_ens_outputs$Q95)))

p1 = ggplot(data = plt_model_outputs, 
            aes(x = target_date)) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                  filter(location == "US", 
                         time_idx >= "2022-10-01",
                         time_idx <= "2023-03-01"),
              aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_ribbon(aes(ymin = Q5, ymax = Q95), alpha = 0.2) + 
    geom_ribbon(aes(ymin = Q25, ymax = Q75), alpha = 0.2) + 
    geom_line(aes(y = Q50), size = 0.9) +
    facet_wrap(vars(model_id), ncol = 1)+
    scale_y_continuous(labels = comma, 
                       limits = lims,
                       name = "US incident hospitalizations") +
    theme_bw() +
    theme(axis.title.x = element_blank(), 
          legend.position = "none", 
          legend.title = element_blank())
    
p2 = ggplot(data = plt_ens_outputs, 
            aes(x = target_date)) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_ribbon(aes(ymin = Q5, ymax = Q95, fill = model_id), alpha = 0.2) + 
    geom_ribbon(aes(ymin = Q25, ymax = Q75, fill = model_id), alpha = 0.2) + 
    geom_line(aes(y = Q50, color = model_id), size = 0.9) +
    scale_y_continuous(labels = comma, 
                       limits = lims,
                       name = "US incident hospitalizations") +
    theme_bw() +
    theme(axis.title.x = element_blank(), 
          legend.position = "bottom", 
          legend.title = element_blank())

plot_grid(p1, p2, 
          align = "h", axis = "b")
```


# Case study: Weekly incident flu hospitalizations

To demonstrate the utility of the `hubEnsembles` package and the differences between the two ensembling functions, we examine the case of predicting weekly influenza hospitalizations in the US. <!-- ELR: I'm commenting out this text, which feels out of place to me. "Accurate infectious disease forecasts are needed to make decisions that inform public policy, balance prevention and mitigation efforts with other monetary and societal costs, and properly allocate resources [@disease_economics]. However, making reliable predictions is hindered by four main factors unique to infectious disease forecasting: 1) the systems governing the spread of infectious disease tend to be non-stationary, 2) communicated forecasts impact the spread of infectious disease, 3) the quality of truth data is often variable, and 4) there are many unknowns about underlying disease mechanisms [@lauer]." -->

For over XX years, the US Center for Disease Control and Prevention (CDC) has been soliciting forecasts of seasonal influenza from modeling teams through a collaborative challenge called FluSight [REF]. Here we combine these forecasts using various aggregation methods to take advantage of the greater consistency [@hibon2005] and accuracy [@clemen1989; @timmermann2006b] of ensembles over individual models. In particular, we examine four equally-weighted ensembling methods implemented through `simple_ensemble()` and `linear_pool()`: a quantile (arithmetic) mean, a quantile median, a linear pool with normal tails, and a linear pool with lognormal tails.

The component forecasts used to generate the ensembles consist of those submitted during influenza seasons 2021-2022 and 2022-2023, the only two complete seasons for which FluSight collected quantile forecasts for a target of weekly incident flu hospitalizations at the time of this writing. These predictions are not yet formatted according to hubverse standards but are easily transformed using the `as_model_out_tbl()` function from the `hubUtils` package.

<!-- ELR: Let's think about whether we can remove the dependence on covidHubUtils. I filed an issue for this here: https://github.com/Infectious-Disease-Modeling-Hubs/hubEnsembles/issues/46 -->

```{r transform data, eval=FALSE}
forecast_data_hub <- flu_data_raw |>
  covidHubUtils::as_covid_hub_forecasts(model_id_col = "model_id",
                                        reference_date_col="timezero", 
                                        location_col="unit", 
                                        horizon_col="horizon", 
                                        target_col="target_long", 
                                        output_type_col="output_type",
                                        output_type_id_col="output_type_id",
                                        value_col="value",
                                        temp_res_col=NULL, 
                                        target_end_date_col=NULL)
```

<!-- consider showing the head of the data frame? -->

Within the now-properly formatted model outputs, the task ID variables are horizon, location, target, and forecast date (the date on which the forecast was made). All of the forecasts have a quantile output type with 23 total unique output type IDs $$Q = \{.010, 0.025, .050, .100, \cdots, .900, .950, .990\}.$$ The values of these quantile forecasts are non-negative. The resulting ensemble forecasts will have the same task ID variables and model output specifications.

Next the component model outputs are combined using the following code to generate each model.

```{r construct ensembles, eval=FALSE}
mean_ensemble <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::simple_ensemble(weights=NULL, agg_fun = "mean", model_id="mean-ensemble") 

median_ensemble <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::simple_ensemble(weights=NULL, agg_fun = "median", model_id="median-ensemble")
  
lp_normal <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::linear_pool(weights=NULL, n_samples = 1e5, model_id="lp-normal", tail_dist="norm") 

lp_lognormal <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::linear_pool(weights=NULL, n_samples = 1e5, model_id="lp-lognormal", tail_dist="lnorm") 
```

```{r read in forecasts and scores, echo=FALSE}
library(covidHubUtils)
library(lubridate)
library(patchwork)
source("evaluation_functions.R")

flu_truth_all <- readr::read_rds("data/flu_truth_all.rds")
flu_files <- list.files(path="data", pattern="hub", full.names=TRUE)
flu_forecasts_all <- purrr::map_dfr(flu_files, .f=readr::read_rds) |>
  covidHubUtils::as_covid_hub_forecasts(model_id_col = "model_id",
                        reference_date_col="forecast_date", 
                        location_col="location", 
                        horizon_col="horizon", 
                        target_col="target", 
                        temp_res_col=NULL, 
                        target_end_date_col="target_end_date")

flu_scores_baseline <- readr::read_rds("data/flu_baseline_scores.rds")
flu_scores_ensembles <- readr::read_rds("data/flu_scores_ensembles.rds")
flu_scores_all <- rbind(flu_scores_ensembles, flu_scores_baseline)

flu_dates_21_22 <- as.Date("2022-01-24") + weeks(0:21)
flu_dates_22_23 <- as.Date("2022-10-17") + weeks(0:30)
all_flu_dates <- c(flu_dates_21_22, flu_dates_22_23)
```

We score the ensemble forecasts for every unique combination of task ID variables using several common metrics in forecast evaluation, including weighted interval score (WIS) [@bracher_evaluating_2021], mean absolute error (MAE), 50% prediction interval (PI) coverage, and 95% PI coverage. Of these metrics, WIS and PI coverage evaluate probabilistic forecasts while MAE evaluates point forecasts. In this analysis, we take the 0.5 quantile to be the point forecast. 

WIS measures how consistent a set of prediction intervals is with the true value and is an alternative to common proper scoring rules like the Log Score and Continuous Ranked Probability Score, which can't be evaluated directly for quantile forecasts [@bracher_evaluating_2021]. Since WIS is made up of three component penalties—one for each of spread, over-prediction, and under-prediction—a lower value indicates a more accurate forecast [@bracher_evaluating_2021]. The $(1-\alpha)*100$% PI coverage measures the proportion of the time that PIs at that nominal level included the true value, which provides information about whether a forecast has accurately characterized the uncertainty of future observations. Achieving approximately nominal ($(1-\alpha)*100$%) coverage indicates a well-calibrated forecast. MAE measures the average absolute error of a set of forecasts against the true value; smaller values of MAE indicate better forecast accuracy.

<!-- ELR: commenting out the following text.  Rather than saying "we made tables and figures with results", let's say things like "The results show that XX model was better than YY model (Table 1, Fig 2)".   "The scores are then grouped together and averages are taken of these metrics over different combinations of five variables: none, season, horizon, forecast date, and location. We display a table of the overall results (no grouping) and figures showing average WIS, MAE, and 95% PI coverage plotted broken down by forecast date, horizon, and season." -->

The quantile median ensemble had the best overall performance in terms of WIS and MAE (and the relative versions of these metrics) with above-nominal coverage rates (Table 1). The two linear opinion pools had very similar performance to each other. These methods had the second-best performance as measured by WIS and MAE, but they had the highest 50% and 95% coverage rates, with empirical coverage that was well above the nominal coverage rate. The quantile mean performed the worst of the ensembles.

```{r overall evaluation, message=FALSE, warning=FALSE, echo=FALSE}
flu_overall_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=NULL, baseline_name="Flusight-baseline", us_only=FALSE)

knitr::kable(flu_overall_states, caption="Summary of overall model performance across both seasons, averaged over all locations except the US national location.")
```

Plots of the models' forecasts can aid our understanding about the origin of these accuracy differences. For example, the linear opinion pools, which had the highest coverage rates, consistently had some of the widest prediction intervals. The median ensemble, which had the best WIS, seems to have best-balanced interval width overall, with narrower intervals than the linear pools that achieved near-nominal coverage on average across all time points. The quantile mean frequently suffered from narrow intervals, though at times it had intervals as wide or wider than those of the linear pools. <!-- ELR I deleted some stuff about point prediction accuracy in this paragraph. I think it could be worthwhile commenting on point prediction accuracy, but I wasn't really convinced by the claims that were being made. To my eye, a lot of the time the point predictions look similarly bad for all models (e.g. coming out of the peak of the 2022/2023 season, the point predictions from all methods look pretty similar).  One thing that does stand out is a failure of the mean point forecasts in HI in the first season. See comment below, but I wonder if we could find a more impactful (i.e. in a state with a larger population and therefore a larger contribution to WIS) example of that kind of failure? --> 

<!-- ELR: comments on the plot below:
 - consider using hubVis for the plot
 - can we get it so that there is not a line during the off season in this plot?
 - I don't think we need to color by model or have a legend for model color, since the model names are already in the facet labels.
 - From previous investigations, my understanding is that a problem with the quantile mean ensemble is that it can go off the rails when one forecast is very bad. You can see something like this happening in HI, in the beginning of the first season. That particular example is not going to impact MAE and WIS scores, though, because the counts in HI are small. Is there a chance that there is another location that has similar behavior that might be driving a part of this failure of the quantile mean ensemble?
-->

```{r plot forecasts, echo = FALSE}
flu_forecasts_wide <- flu_forecasts_all |>
  dplyr::left_join(hub_locations_flusight, by = c("location" = "fips")) |>
  dplyr::mutate(horizon=as.character(horizon)) 

select_dates <- all_flu_dates[seq(1, 53, 4)]


forecasts_hi <- flu_forecasts_wide %>% 
  filter(location == "15", forecast_date %in% select_dates)
    
hi_plot <- plot_forecasts(forecasts_hi, 
                    hub = "FluSight",
                    intervals = c(.50, .95),
                    truth_data = select(flu_truth_all, model, location, target_end_date, target_variable, value),
                    truth_source = "HealthData",
                    use_median_as_point = TRUE,
                    facet = model ~.,
                    facet_nrow = 5,
                    fill_by_model = TRUE, 
                    title = "none",
                    subtitle = "none",
                    show_caption = FALSE,
                    plot=FALSE)

max_truth_hi <- max(filter(flu_truth_all, location=="15")$value)
max_forecast_hi <- max(filter(forecasts_hi, location=="15")$value)
  
hi_plot <- hi_plot +
  scale_x_date(name=NULL, limits = c(as.Date("2022-01-01"), as.Date("2023-06-08")), date_labels = "%b '%y") +
  coord_cartesian(ylim = c(0, max(25, min(max_truth_hi * 3.5, max_forecast_hi)))) +
  ggtitle("Hawaii (lowest cumulative hospitalizations)") +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")
          

forecasts_tx <- flu_forecasts_wide %>% 
  filter(location == "48", forecast_date %in% select_dates)
    
tx_plot <- plot_forecasts(forecasts_tx, 
                    hub = "FluSight",
                    intervals = c(.50, .95),
                    truth_data = select(flu_truth_all, model, location, target_end_date, target_variable, value),
                    truth_source = "HealthData",
                    use_median_as_point = TRUE,
                    facet = model ~.,
                    facet_nrow = 5,
                    fill_by_model = TRUE, 
                    title = "none",
                    subtitle = "none",
                    show_caption = FALSE,
                    plot=FALSE)

max_truth_tx <- max(filter(flu_truth_all, location=="48")$value)
max_forecast_tx <- max(filter(forecasts_tx, location=="48")$value)
  
tx_plot <- tx_plot +
  scale_x_date(name=NULL, limits = c(as.Date("2022-01-01"), as.Date("2023-06-08")), date_labels = "%b '%y") +
  coord_cartesian(ylim = c(0, max(25, min(max_truth_tx * 3.5, max_forecast_tx)))) +
  ggtitle("Texas (highest cumulative hospitalizations)") +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")
          

hi_plot + tx_plot +
  plot_layout(ncol = 2, guides='collect') &
  plot_annotation(
    title="Weekly Influenza Hospitalizations: observed and forecasted",
    subtitle="Forecasts for Hawaii and Texas, every 4 weeks",
  ) &
  theme(legend.position='bottom')
```

<!-- ELR similar to above, removing text that says "a figure exists" so that we can jump more directly to the results. Commented text: "We may also examine the scores grouped by forecast date and horizon. The three sets of plots show average WIS, MAE, and 95% PI coverage for these groupings (as well as by season for readability)." --> The ensemble models all have similar MAE during the entire time period for the one week ahead horizon, though the MAE values diverge at increased horizons [ref to figure]. However, the models show greater differences for the other two metrics, especially during times of rapid change [ref to figures]. Specifically, during ___, the linear pools have better WIS than the median ensemble at the one week ahead forecast horizon. The PI coverage rates for the linear pools were at least as large as the coverage rates of the other models throughout the entire period of analysis at both the 1 and 4 week ahead forecast horizons.

<!-- ELR: thoughts about the plot below:
 - I think it would be helpful to add panels that show the actual time series data, as a reference for claims about when incidence is rising.
 - But the challenge is that then, you'll have an unmanageable number of panels.... Maybe it would help to just split the figure into three different figures or subfigures, one for each of the three metrics (MAE, WIS, PI coverage)? With the seasons next to each other, i.e., column 1 for 2021-2022 and column 2 for 2022-2023. This could also help to just make the whole thing more readable. r.e. readability, see also Harry's comments in a github issue.
 - For the 4 week ahead results in the 2022-2023 season, it looks like at least one model is getting cut off.
-->

```{r By date States metrics, message=FALSE, warning=FALSE, fig.cap='Average h-week ahead WIS, MAE, and 95% PI coverage for each model for all locations except the US national location.', echo=FALSE}  
model_names <- sort(c("Flusight-baseline", "mean-ensemble", "median-ensemble", "lp-normal", "lp-lognormal"))
model_colors <- c("#6BAED6", "#FD8D3C", "#74C476", "#9E9AC8", "#FB6A4A")

flu_date_horizon_season_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

# WIS
wis_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="WIS 2021-2022, 1 week ahead")
wis_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="WIS 2021-2022, 4 week ahead")
  
wis_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="WIS 2022-2023, 1 week ahead")
wis_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="WIS 2022-2023, 4 week ahead")

# MAE
flu_date_horizon_season_states <- flu_scores_all |>
    evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

mae_date_plot_states1_2122 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2021-2022") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="mae", main="MAE 2021-2022, 1 week ahead")
mae_date_plot_states4_2122 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2021-2022") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="mae", main="MAE 2021-2022, 4 week ahead")

mae_date_plot_states1_2223 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2022-2023") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="mae", main="MAE 2022-2023, 1 week ahead")
mae_date_plot_states4_2223 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2022-2023") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="mae", main="MAE 2022-2023, 4 week ahead")

# 95% Coverage
cov95_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="cov95", main="95% Coverage 2021-22, 1 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="cov95", main="95% Coverage 2021-22, 4 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
  
cov95_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="cov95", main="95% Coverage 2022-23, 1 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="cov95", main="95% Coverage 2022-23, 4 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
    

wis_date_plot_states1_2122 + mae_date_plot_states1_2122 + cov95_date_plot_states1_2122 + 
  wis_date_plot_states4_2122 + mae_date_plot_states4_2122 + cov95_date_plot_states4_2122 + 
  wis_date_plot_states1_2223 + mae_date_plot_states1_2223 + cov95_date_plot_states1_2223 + 
  wis_date_plot_states4_2223 + mae_date_plot_states4_2223 + cov95_date_plot_states4_2223 + 
  plot_layout(ncol=3, guides='collect') &
  theme(legend.position='bottom')
```

<!-- ELR: I'm not getting much out of this paragraph.  Commenting out for now... "Grouping the scores by horizon on its own and horizon in conjunction with season reveal similar patterns in model rankings, with the quantile median achieving the lowest WIS except at the 1 week ahead horizon. When the scores are grouped by only location, the quantile median has the lowest relative WIS for all but three locations, two of which demonstrate inferior performance compared to the baseline and one which has worse performance than the quantile mean. The linear pools outperform the quantile mean ensemble for 75% of locations and perform the same for about 8%, each of these three models outperforming the baseline for approximately 75% of locations." -->

From these results, we can see that different ensembling methods perform best under different circumstances, though in this analysis all of the ensemble variations outperformed the baseline model. While the quantile median had the best overall results for WIS, MAE, 50% PI coverage, and 95% PI coverage, other models may perform better from week-to-week for each metric. For example, the linear pools achieved the lowest WIS during several instances of rapid change in incident flu hospitalizations. <!-- ELR: "several instances"? I think I saw discussion above about one period like this, let's specifically call out others too. -->

Depending on the target being forecast or the goal of forecasting, the user may opt to choose a particular aggregation method that is most aligned with the objectives. One case may call for prioritizing above-nominal coverage rates while another may be more interested in accurate point forecasts. The `simple_ensemble` and `linear_pool` functions and the ability to specify component model weights and an aggregation function for `simple_ensemble` allow users to implement a variety of ensemble methods.


# Conclusion
<!-- # EH: I opted for short and sweet here. We could also expand each of the ideas into a paragraph. -->

Ensembles of independent models are a powerful tool to generate more accurate and more reliable forecasts of future outcomes than a single model alone. Here, we have demonstrated how to utilize `hubEnsembles`, a simple and flexible framework to combine individual model forecasts and create ensemble predictions. When using `hubEnsembles`, it is important to carefully choose an ensemble method that is well suited for the situation. Although there may not be a "best" method, matching the properties of a given ensemble method with the features of the component models will likely yield best results. For example, we showed for forecasts of seasonal influenza in the US, the quantile median ensemble performed best overall, but the linear pool method had advantages during periods of rapid change, when outlying component forecasts were likely more important. Importantly, all ensemble methods outperformed the baseline model. These performance improvements from ensemble models motivate the use of a "hub-based" approach to prediction for infectious diseases and in other fields. Fitting within the larger suite of "hubverse" tools that support such efforts, the `hubEnsembles` package provides important software infrastructure for leveraging the power of multi-model ensembles.

# References